{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEP-8 не работает..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как пип 8 не работает и никто не помогает, я его закомментировала("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycodestyle in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.5.0)\n",
      "Requirement already satisfied: flake8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (3.7.8)\n",
      "Requirement already satisfied: pycodestyle_magic in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.4)\n",
      "Requirement already satisfied: pyflakes<2.2.0,>=2.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from flake8) (2.1.1)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from flake8) (0.6.1)\n",
      "Requirement already satisfied: entrypoints<0.4.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from flake8) (0.3)\n",
      "Requirement already satisfied: pycodestyle in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.5.0)\n",
      "Requirement already satisfied: flake8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (3.7.8)\n",
      "Requirement already satisfied: pycodestyle_magic in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.4)\n",
      "Requirement already satisfied: pyflakes<2.2.0,>=2.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from flake8) (2.1.1)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from flake8) (0.6.1)\n",
      "Requirement already satisfied: entrypoints<0.4.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from flake8) (0.3)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install pycodestyle flake8 pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%load_ext pycodestyle_magic\n",
    "#%pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала импортируем необходимые для выполнения заданий модули.Затем открываем наш файл и читаем его. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kseniapetuhova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kseniapetuhova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "from pprint import pprint\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import collections\n",
    "m = Mystem()\n",
    "with open('we.txt', encoding='utf-8') as fh:\n",
    "    text = fh.read()\n",
    "a = len(text)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь засекаем время и парсим текст с помощью Mystem()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 366 ms, sys: 75.9 ms, total: 442 ms\n",
      "Wall time: 5.45 s\n",
      "CPU times: user 366 ms, sys: 75.9 ms, total: 442 ms\n",
      "Wall time: 5.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ana = m.analyze(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем файл формата json, в который кладем наш объект питона. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(ana, f, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Засекаем время, токенизируем текст, затем для каждого слова из получившихся токенов применяем разбор pymorphy. Чтобы созранить результат в файл с форматом json, создаем словарь, ключами которого будут слова, а значениями - леммы и части речи этих слов. Создаем файл и записываем туда наш питоновский словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 s, sys: 31 ms, total: 14.4 s\n",
      "Wall time: 14.4 s\n",
      "CPU times: user 14.4 s, sys: 31 ms, total: 14.4 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemmas = {}\n",
    "token = word_tokenize(text)\n",
    "for word in token:\n",
    "    ana = morph.parse(word)\n",
    "    ana = ana[0]\n",
    "    lemmas[ana.word] = ana.normal_form,ana.tag.POS\n",
    "with open('task_3.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(lemmas, f, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем список частей речи, которые берем из документации pymorphy для русского языка. И для каждого элемента этого списка проверяем, есть ли оно в разборе наших слов. Для каждого элемента заводим переменную, которая увеличивается на 1 когда часть речи встречается в разборе, после чего делим значение этой переменной для каждой части речи на количество токенов и умножаем на 100, чтобы получить проценты. Но так как среди токенов встречаются знаки пунктуации, а времени до дедлайна очень мало, чтобы что-то предпринять, то у меня в сумме не получается 100%((((("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN = 15.780451569719034\n",
      "NOUN = 15.780451569719034\n",
      "ADJF = 9.901555842162834\n",
      "ADJF = 9.901555842162834\n",
      "ADJS = 0.7738396486702692\n",
      "ADJS = 0.7738396486702692\n",
      "COMP = 0.42283643250126524\n",
      "COMP = 0.42283643250126524\n",
      "VERB = 8.246126720323902\n",
      "VERB = 8.246126720323902\n",
      "INFN = 1.4333991804483046\n",
      "INFN = 1.4333991804483046\n",
      "PRTF = 0.941994677811699\n",
      "PRTF = 0.941994677811699\n",
      "PRTS = 0.27753742673828224\n",
      "PRTS = 0.27753742673828224\n",
      "GRND = 0.37712438574437174\n",
      "GRND = 0.37712438574437174\n",
      "NUMR = 0.32161690039671526\n",
      "NUMR = 0.32161690039671526\n",
      "ADVB = 6.3115275986482295\n",
      "ADVB = 6.3115275986482295\n",
      "NPRO = 7.480449937145936\n",
      "NPRO = 7.480449937145936\n",
      "PRED = 0.4979347950304475\n",
      "PRED = 0.4979347950304475\n",
      "PREP = 7.031492335069303\n",
      "PREP = 7.031492335069303\n",
      "CONJ = 7.679623855158114\n",
      "CONJ = 7.679623855158114\n",
      "PRCL = 5.013631985372145\n",
      "PRCL = 5.013631985372145\n",
      "INTJ = 0.1681550291414298\n",
      "INTJ = 0.1681550291414298\n"
     ]
    }
   ],
   "source": [
    "list_of_pos = ['NOUN', 'ADJF', 'ADJS', 'COMP', 'VERB', 'INFN', 'PRTF', \n",
    "               'PRTS', 'GRND', 'NUMR', 'ADVB', 'NPRO', 'PRED', 'PREP',\n",
    "               'CONJ', 'PRCL', 'INTJ']\n",
    "for x in list_of_pos:\n",
    "    counter = 0\n",
    "    for word in token:\n",
    "        ana = morph.parse(word)\n",
    "        ana = ana[0]\n",
    "        if x in ana.tag:\n",
    "            counter += 1\n",
    "    print(x, '=', counter/(len(token)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем пустой список. В разборе каждого нашего слова смотрим, является ли оно глаголом. Если является, то записываем его в список. Затем из нашего списка создаем частотный словарь, сортируем его по значениям и выводим топ-20 глаголов. То же самое повторяем для наречий. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "топ-20 глаголов:\n",
      "топ-20 глаголов:\n",
      "быть : 447\n",
      "мочь : 232\n",
      "знать : 166\n",
      "видеть : 115\n",
      "хотеть : 83\n",
      "сказать : 82\n",
      "понимать : 78\n",
      "говорить : 65\n",
      "увидеть : 56\n",
      "чувствовать : 55\n",
      "идти : 51\n",
      "помнить : 47\n",
      "слышать : 47\n",
      "думать : 39\n",
      "стать : 39\n",
      "понять : 31\n",
      "смотреть : 29\n",
      "сидеть : 23\n",
      "нумереть : 22\n",
      "остаться : 22\n",
      "топ-20 наречий:\n",
      "быть : 447\n",
      "мочь : 232\n",
      "знать : 166\n",
      "видеть : 115\n",
      "хотеть : 83\n",
      "сказать : 82\n",
      "понимать : 78\n",
      "говорить : 65\n",
      "увидеть : 56\n",
      "чувствовать : 55\n",
      "идти : 51\n",
      "помнить : 47\n",
      "слышать : 47\n",
      "думать : 39\n",
      "стать : 39\n",
      "понять : 31\n",
      "смотреть : 29\n",
      "сидеть : 23\n",
      "нумереть : 22\n",
      "остаться : 22\n",
      "топ-20 наречий:\n",
      "уже : 145\n",
      "там : 142\n",
      "сейчас : 132\n",
      "ясно : 87\n",
      "вдруг : 83\n",
      "потому : 78\n",
      "теперь : 68\n",
      "потом : 66\n",
      "вниз : 60\n",
      "здесь : 58\n",
      "очень : 56\n",
      "сегодня : 52\n",
      "опять : 52\n",
      "медленно : 50\n",
      "тут : 49\n",
      "тогда : 49\n",
      "где : 46\n",
      "совершенно : 46\n",
      "завтра : 46\n",
      "всего : 45\n",
      "уже : 145\n",
      "там : 142\n",
      "сейчас : 132\n",
      "ясно : 87\n",
      "вдруг : 83\n",
      "потому : 78\n",
      "теперь : 68\n",
      "потом : 66\n",
      "вниз : 60\n",
      "здесь : 58\n",
      "очень : 56\n",
      "сегодня : 52\n",
      "опять : 52\n",
      "медленно : 50\n",
      "тут : 49\n",
      "тогда : 49\n",
      "где : 46\n",
      "совершенно : 46\n",
      "завтра : 46\n",
      "всего : 45\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "a = 0\n",
    "b = 0\n",
    "list_verbs = []\n",
    "print('топ-20 глаголов:')\n",
    "for word in token:\n",
    "        ana = morph.parse(word)\n",
    "        ana = ana[0]\n",
    "        if 'VERB' in ana.tag:\n",
    "            list_verbs.append(ana.normal_form)\n",
    "counter = collections.Counter(list_verbs)\n",
    "counter = dict(counter)\n",
    "top_verbs = list(counter.items())\n",
    "top_verbs.sort(key=lambda i: i[1], reverse=True)\n",
    "for i in top_verbs:\n",
    "    if a < 20:\n",
    "        print(i[0], ':', i[1])\n",
    "        a += 1 \n",
    "list_advb = []\n",
    "print('топ-20 наречий:')\n",
    "for word in token:\n",
    "        ana = morph.parse(word)\n",
    "        ana = ana[0]\n",
    "        if 'ADVB' in ana.tag:\n",
    "            list_advb.append(ana.normal_form)\n",
    "counter_2 = collections.Counter(list_advb)\n",
    "counter_2 = dict(counter_2)\n",
    "top_advb = list(counter_2.items())\n",
    "top_advb.sort(key=lambda i: i[1], reverse=True)\n",
    "for i in top_advb:\n",
    "    if b < 20:\n",
    "        print(i[0], ':', i[1])\n",
    "        b += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем пустой список. Затем для каждого токена првоеряем, не является ли он знаком препинания. Если нет, то делаем его разбор с помощью pymorphy. В наш пустой список добавлем леммы разобранных слов. С помощью специальной команды nltk.bigrams() получаем биграммы и кладем их в список, из которого создаем частотный словарь, сортируем и выводим топ-25 биграмм. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('и', 'я') : 135\n",
      "('я', 'не') : 111\n",
      "('у', 'я') : 93\n",
      "('мочь', 'быть') : 91\n",
      "('не', 'мочь') : 85\n",
      "('что', 'я') : 75\n",
      "('не', 'знать') : 72\n",
      "('это', 'быть') : 69\n",
      "('и', 'весь') : 67\n",
      "('потому', 'что') : 66\n",
      "('и', 'вот') : 63\n",
      "('весь', 'это') : 60\n",
      "('я', 'быть') : 60\n",
      "('в', 'я') : 56\n",
      "('единый', 'государство') : 50\n",
      "('и', 'не') : 48\n",
      "('не', 'быть') : 47\n",
      "('если', 'бы') : 47\n",
      "('я', 'в') : 47\n",
      "('я', 'и') : 45\n",
      "('я', 'знать') : 44\n",
      "('я', 'видеть') : 44\n",
      "('так', 'же') : 42\n",
      "('и', 'в') : 42\n",
      "('на', 'я') : 40\n",
      "('и', 'я') : 135\n",
      "('я', 'не') : 111\n",
      "('у', 'я') : 93\n",
      "('мочь', 'быть') : 91\n",
      "('не', 'мочь') : 85\n",
      "('что', 'я') : 75\n",
      "('не', 'знать') : 72\n",
      "('это', 'быть') : 69\n",
      "('и', 'весь') : 67\n",
      "('потому', 'что') : 66\n",
      "('и', 'вот') : 63\n",
      "('весь', 'это') : 60\n",
      "('я', 'быть') : 60\n",
      "('в', 'я') : 56\n",
      "('единый', 'государство') : 50\n",
      "('и', 'не') : 48\n",
      "('не', 'быть') : 47\n",
      "('если', 'бы') : 47\n",
      "('я', 'в') : 47\n",
      "('я', 'и') : 45\n",
      "('я', 'знать') : 44\n",
      "('я', 'видеть') : 44\n",
      "('так', 'же') : 42\n",
      "('и', 'в') : 42\n",
      "('на', 'я') : 40\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "bigr_list = []\n",
    "for tok in token:\n",
    "    if tok not in \"\"\"!#$%^&*()_+:<>?,./\\|-;\"\"\":\n",
    "        if tok != '...' and tok != '--':\n",
    "            if tok != '``' and tok != \"''\":\n",
    "                needed = morph.parse(tok)\n",
    "                bigr_list.append(needed[0].normal_form)\n",
    "bigrm = list(nltk.bigrams(bigr_list))\n",
    "counter = collections.Counter(bigrm)\n",
    "counter = dict(counter)\n",
    "top_bi = list(counter.items())\n",
    "top_bi.sort(key=lambda i: i[1], reverse=True)\n",
    "for i in top_bi:\n",
    "    if a < 25:\n",
    "        print(i[0], ':', i[1])\n",
    "        a += 1        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для триграмм делаем все тоже самое, только вместо nltk.bigrams() используем nltk.trigrams()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('я', 'не', 'мочь') : 24\n",
      "('так', 'же', 'как') : 13\n",
      "('один', 'и', 'тот') : 13\n",
      "('и', 'тот', 'же') : 13\n",
      "('я', 'не', 'знать') : 13\n",
      "('в', 'самый', 'дело') : 12\n",
      "('и', 'тотчас', 'же') : 11\n",
      "('у', 'я', 'в') : 11\n",
      "('до', 'сей', 'пора') : 11\n",
      "('в', 'древний', 'дом') : 11\n",
      "('если', 'бы', 'я') : 11\n",
      "('там', 'за', 'стена') : 10\n",
      "('не', 'знать', 'что') : 10\n",
      "('и', 'я', 'не') : 10\n",
      "('я', 'не', 'хотеть') : 10\n",
      "('потому', 'что', 'я') : 10\n",
      "('один', 'и', 'то') : 9\n",
      "('и', 'то', 'же') : 9\n",
      "('или', 'мочь', 'быть') : 9\n",
      "('и', 'я', 'видеть') : 9\n",
      "('раз', 'в', 'жизнь') : 8\n",
      "('у', 'я', 'быть') : 8\n",
      "('в', 'один', 'и') : 8\n",
      "('и', 'не', 'мочь') : 8\n",
      "('вы', 'неведомый', 'мой') : 8\n",
      "('я', 'не', 'мочь') : 24\n",
      "('так', 'же', 'как') : 13\n",
      "('один', 'и', 'тот') : 13\n",
      "('и', 'тот', 'же') : 13\n",
      "('я', 'не', 'знать') : 13\n",
      "('в', 'самый', 'дело') : 12\n",
      "('и', 'тотчас', 'же') : 11\n",
      "('у', 'я', 'в') : 11\n",
      "('до', 'сей', 'пора') : 11\n",
      "('в', 'древний', 'дом') : 11\n",
      "('если', 'бы', 'я') : 11\n",
      "('там', 'за', 'стена') : 10\n",
      "('не', 'знать', 'что') : 10\n",
      "('и', 'я', 'не') : 10\n",
      "('я', 'не', 'хотеть') : 10\n",
      "('потому', 'что', 'я') : 10\n",
      "('один', 'и', 'то') : 9\n",
      "('и', 'то', 'же') : 9\n",
      "('или', 'мочь', 'быть') : 9\n",
      "('и', 'я', 'видеть') : 9\n",
      "('раз', 'в', 'жизнь') : 8\n",
      "('у', 'я', 'быть') : 8\n",
      "('в', 'один', 'и') : 8\n",
      "('и', 'не', 'мочь') : 8\n",
      "('вы', 'неведомый', 'мой') : 8\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "trigr_list = []\n",
    "for tok in token:\n",
    "    if tok not in \"\"\"!#$%^&*()_+:<>?,./\\|-;\"\"\":\n",
    "        if tok != '...' and tok != '--':\n",
    "            if tok != '``' and tok != \"''\":\n",
    "                needed = morph.parse(tok)\n",
    "                trigr_list.append(needed[0].normal_form)\n",
    "trigrm = list(nltk.trigrams(trigr_list))\n",
    "counter = collections.Counter(trigrm)\n",
    "counter = dict(counter)\n",
    "top_tri = list(counter.items())\n",
    "top_tri.sort(key=lambda i: i[1], reverse=True)\n",
    "for i in top_tri:\n",
    "    if a < 25:\n",
    "        print(i[0], ':', i[1])\n",
    "        a += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
